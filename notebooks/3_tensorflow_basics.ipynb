{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow baiscs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and compute graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are multidimensional matrices. It is the core datatype in TensorFlow.\n",
    "\n",
    "Since TF2.0, TensorFlow tensors works very similar to numpy arrays, the most basics type of tensors is a `tf.constant`, you can create a tensor from a number, or a numpy array. You can even convert them to arrays using `tensor.numpy()`.\n",
    "\n",
    "TensorFlow also has many numpy-like functions to create or do tensorial computations.\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(np.linspace(0,1,11)) # from numpy array\n",
    "c = tf.ones([3, 3]) # a 10x20 matrix\n",
    "\n",
    "\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('c:', c)\n",
    "print('c as numpy:', c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of TensorFlow comes from graphs. When ever you compute something \n",
    "using tensors, TensorFlow records what you have done by constructing a \n",
    "graph of computations.\n",
    "\n",
    "![static_graph](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "\n",
    "One of purpose of doing this is that you can now compute the gradients of \n",
    "$y=f(x)$ with respect to $x$, using the chain rule of differentiation. \n",
    "So long as $x$ is computed  from $y$, you can trace back each operation back to\n",
    "$x$ from $y$\n",
    "\n",
    "To do this, you use the `GradientTape` to tell TensorFlow what are you interested\n",
    "in as $x$. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(5.0)\n",
    "\n",
    "# Inside the \"with\" call, we compute y from x\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "# After we have y, we can have the gradient dy/dx\n",
    "# which is \"recorded\" by the tape\n",
    "dydx = tape.gradient(y, x)\n",
    "\n",
    "print(dydx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the number make sense? You can try a few more times to see what it does.  \n",
    "\n",
    "**TASK:** Try to find out:\n",
    "- Can a tape record many x variables?\n",
    "  (then we want the partial derivatives)\n",
    "- Can it compute second order gradients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "We now have the power to compute gradients of any function, \n",
    "so long as we can compute that function in TensorFlow. \n",
    "The natural usage of this is to implement the gradient descent algorithm.\n",
    "*You have used it before, remember?*\n",
    "\n",
    "We showcase this with the simple linear regression problem, trying to fit some data with the function:\n",
    "$$y_{pred} = kx + b$$\n",
    "\n",
    "We try to minimize the loss function:\n",
    "$$loss = (y_{pred} - y_{data})^2$$\n",
    "\n",
    "Suppose the data we'd like to fit looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_data = np.linspace(0,10,20)\n",
    "y_data = x_data *2 + 1\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, you need to \n",
    "\n",
    "- define two variables: `k` and `b`;\n",
    "- compute `loss` function with a GradientTape;\n",
    "- compute the gradient of `loss` with respect to k and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.Variable(4.0)#\n",
    "b = tf.Variable(2.0)#\n",
    "\n",
    "# note we added persisten=True here\n",
    "# this allows you to do gradient multiple times\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # You need to complet the code from here\n",
    "    tape.watch(k)\n",
    "    tape.watch(b)\n",
    "\n",
    "    # to here \n",
    "    y_pred = x_data * k + b\n",
    "    loss = tf.reduce_mean((y_pred - y_data)**2)\n",
    "\n",
    "dlossdk = tape.gradient(loss, k)\n",
    "dlossdb = tape.gradient(loss, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is simple, if we know the gradient of the loss function (that is, \n",
    "the error of prediction) with respect to the variables,\n",
    "we can minimize the error by going to the direction \n",
    "where loss function decreases.\n",
    "\n",
    "![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
    "\n",
    "We then update the values of k and b according to the gradients, in each step, \n",
    "we update the values of k and b according to $dloss/dk$ and $dloss/db$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.Variable(2.0)#\n",
    "b = tf.Variable(2.0)#\n",
    "\n",
    "max_steps = 1000\n",
    "step = 0.01\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # You need to complete the code from here\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(k)\n",
    "        tape.watch(b)\n",
    "        y_pred = x_data * k + b\n",
    "        loss = tf.reduce_mean((y_pred - y_data)**2)    \n",
    "    dlossdk = tape.gradient(loss, k)\n",
    "    dlossdb = tape.gradient(loss, b)\n",
    "    # to here\n",
    "    k = k - step * dlossdk\n",
    "    b = b - step * dlossdb\n",
    "    if i%100 == 0:\n",
    "        print(f'Step={i}, k={k:.2f}, b={b:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** complete the block above to perform the gradient descent.\n",
    "Use the block below to see the result\n",
    "\n",
    "Did it work? Remember that gradient descent can be unstable when the step is large, \n",
    "modify the `max_steps` and `step` to see if it helps?\n",
    "\n",
    "**BONUS**: store the loss function of each step and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_pred)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Of course, we are not satisfied with linear regression,\n",
    "or any regression with a known function. \n",
    "In the real work scenario we cannot even have a good guess of how a function looks like.\n",
    "This is when neural network could save the day.\n",
    "\n",
    "\n",
    "First, let's define our \"very complex\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0,10,20)\n",
    "y_data = np.sin(x_data)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure\n",
    "The idea of a single-layer neural network is that we take the input\n",
    "variable, transform it into several hidden units with a linear\n",
    "function and an activation function. And predict the output\n",
    "as a linear combination of the hidden layers.\n",
    "\n",
    "<img width=400px src=\"https://miro.medium.com/max/1000/1*_7Om4rgZytZe10fXUZkNCA.png\" \n",
    "     style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "### The operations\n",
    "\n",
    "Mathematically, this means you need `n_hidden` $k_1$ values called\n",
    "\"weights\", and `n_hidden` $b$ values called \"biases\" to transform the \n",
    "input to hidden units. Then `n_hidden` $k_2$ values and one $b_2$\n",
    "to get the output.\n",
    "\n",
    "Written in the vector form:\n",
    "- $\\vec{hidden} = tanh(\\vec{k_1}\\cdot x+\\vec{b_1})$\n",
    "- $y = \\vec{k_2}^T\\cdot \\vec{hidden}+b_2$\n",
    "\n",
    "\n",
    "### The intuition\n",
    "The activation function we use here the *tanh* function, \n",
    "it looks like this \n",
    "\n",
    "<img width=400px src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1280px-Hyperbolic_Tangent.svg.png\" \n",
    "     style=\"width: 400px;\"/>\n",
    "          \n",
    "It's an essential part of neural networks, without an activation, this neural network is \n",
    "nothing better than a plain linear regression.\n",
    "\n",
    "An intuitive explanation of how this works is that tanh outputs \"almost\" a linear \n",
    "function between [-0.5, 0.5], and constants when the input is large. It means \n",
    "it only \"activates\" between certain regions of the input. Which a lot of \n",
    "hidden units, we can fit different parts of a function separately.\n",
    "\n",
    "\n",
    "Now we'll implement such a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try to compute the hidden layer from $k_1$ and $b_1$:\n",
    "\n",
    "You may see an error saying that the two shapes are incompatible.\n",
    "This is because you want to compute the 10 multiplications \n",
    "for every data you have in the 20 samples. You therefore \n",
    "need a 2-d matrix represent the hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = tf.random.normal([10])\n",
    "b_1 = tf.random.normal([10])\n",
    "\n",
    "\n",
    "print(x_data.shape, k_1.shape)\n",
    "print(x_data * k_1 + b_1)\n",
    "# ^^^^^ This fails ^^^^\n",
    "\n",
    "print(x_data[:, None].shape, k_1[None, :].shape)\n",
    "hidden = x_data[:, None] * k_1[None, :] + b_1[None, :]\n",
    "print(hidden.shape)\n",
    "# ^^^^ This will work ^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter lines works because TensorFlow (also numpy) broadcasts arrays automatically.  \n",
    "You can read more here: https://www.tensorflow.org/xla/broadcasting.\n",
    "\n",
    "Now try to complete the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = tf.random.normal([500])\n",
    "b1 = tf.random.normal([500])\n",
    "k2 = tf.random.normal([500])\n",
    "b2 = tf.random.normal([1])\n",
    "\n",
    "def neural_net(x, k1, b1, k2, b2):\n",
    "    # You need to complet the code from here\n",
    "    hidden = tf.tanh(x[:, None] * k1[None, :] + b1[None, :])\n",
    "    y_pred = tf.reduce_sum(hidden * k2[None, :], axis=1) + b2\n",
    "    # to here\n",
    "    return y_pred\n",
    "\n",
    "y_pred = neural_net(x_data, k1, b1, k2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with that you can use our previous gradient descent loop to train your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000\n",
    "step = 0.001\n",
    "\n",
    "# You don't need to change the code below,\n",
    "# but feel free to ply with the step settings\n",
    "k1 = tf.random.normal([500])\n",
    "b1 = tf.random.normal([500])\n",
    "k2 = tf.random.normal([500])\n",
    "b2 = tf.random.normal([1])\n",
    "\n",
    "for i in range(max_steps):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(k1)\n",
    "        tape.watch(b1)\n",
    "        tape.watch(k2)\n",
    "        tape.watch(b2)\n",
    "        \n",
    "        y_pred = neural_net(x_data, k1, b1, k2, b2)\n",
    "        loss = tf.reduce_mean((y_pred - y_data)**2)\n",
    "    \n",
    "    dk1 = tape.gradient(loss, k1)\n",
    "    db1 = tape.gradient(loss, b1)\n",
    "    dk2 = tape.gradient(loss, k2)\n",
    "    db2 = tape.gradient(loss, b2)\n",
    "    k1 = k1 - step * dk1\n",
    "    b1 = b1 - step * db1\n",
    "    k2 = k2 - step * dk2\n",
    "    b2 = b2 - step * db2\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(f'Step={i}, loss={loss:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_pred)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** You might see a bad fit, you may now tune the number of hidden units,  \n",
    "the steps of the gradient descent to see if it improves the performance.\n",
    "\n",
    "**Note** that you are also allowed to add more points to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your neural network\n",
    "\n",
    "**TASK:** After you feel happy about your neural network. You should test it \n",
    "on data it has not seen, or a test set.\n",
    "\n",
    "- Does your neural network work well?\n",
    "- What if you try to predict beyond the range you have trained it on?  \n",
    "  (try change `x_test` to something below 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(0,10,1000)\n",
    "y_test = np.sin(x_test)\n",
    "y_pred_test = neural_net(x_test, k1, b1, k2, b2)\n",
    "\n",
    "plt.plot(x_test, y_test, 'r-')\n",
    "plt.plot(x_test, y_pred_test, 'k--')\n",
    "\n",
    "plt.legend(['test set label', 'test set prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network\n",
    "\n",
    "Although it can be proven that a single layer neural network can fit **any** continuous function given enough data, the raise of neural network is due to the advance of deep neural networks (or so-called deep learning).\n",
    "\n",
    "![](https://www.researchgate.net/profile/Martin_Musiol/publication/308414212/figure/fig1/AS:409040078295040@1474534162122/A-general-model-of-a-deep-neural-network-It-consists-of-an-input-layer-some-here-two.png)\n",
    "\n",
    "The idea is rather simple, if transforming the input into a set of non-linear hidden units improves our model compared to a linear one, can we improve more by transforming the hidden units with yet another hidden layer? These models are conceptually possible, but we lack feasible ways to train the parameters.\n",
    "\n",
    "Since we have now $n_{hidden1} \\times n_{hidden2}$ weights to transform hidden layer 1 to hidden layer 2, the number of parameter explodes with number of hidden layers. \n",
    "\n",
    "Thanks to the development of libraries like TensorFlow (to compute the gradients for very deep models), gradient descent (to update the parameters but avoid oscillations)\n",
    "as well as the increasing compute power, the deep neural networks are now much easier to train (aka, to optimize the parameters). \n",
    "\n",
    "\n",
    "**BONUS:** You are now able try to implement a multi-layer neural network using what we've learnt so far. It's Ok if you don't, TensorFlow provides handy functions to help you create deep neural networks efficiently. After the break we will build \n",
    "a deep neural network with the Keras API.\n",
    "\n",
    "Here are some tips if you're going to try:\n",
    "- Mind the dimensions: at this point you will have to do many matrix multiplications, \n",
    "  it is important that you do the then properly and the dimension matches.\n",
    "- Define a new_params = grad_descent(old_params, x_data, y_data)\n",
    "- After you're done, try wrapping the above function with the `@tf.fucntion` \n",
    "  decorator. TensorFlow will automatically optimize your function so that \n",
    "  the computation graph is not generated repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No hints here\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
