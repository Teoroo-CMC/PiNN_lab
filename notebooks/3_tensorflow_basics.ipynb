{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow baiscs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and compute graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors can be seen the multidimensional version of matrices. It is the core datatype in TensorFlow.\n",
    "TensorFlow has many numpy-like functions to create or operate on tensors.\n",
    "\n",
    "Since TF2.0, TensorFlow tensors works very similar to numpy arrays, the most basics type of tensors is a `tf.constant`, you can create a tensor from a number, or a numpy array. You can even convert them to arrays using `tensor.numpy()`.\n",
    "\n",
    "\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(np.linspace(0,1,11)) # from numpy array\n",
    "c = tf.ones([3, 3]) # a 3x3 matrix\n",
    "\n",
    "\n",
    "print('a:', a)\n",
    "print('b:', b)\n",
    "print('c:', c)\n",
    "print('c as numpy:', c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of TensorFlow comes from graphs. When ever you compute something \n",
    "using tensors, TensorFlow records what you have done by constructing a \n",
    "graph of computations.\n",
    "\n",
    "![static_graph](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "\n",
    "One of purpose of doing this is that you can now compute the gradients of \n",
    "$y=f(x)$ with respect to $x$, using the chain rule of differentiation. \n",
    "So long as $x$ is computed  from $y$, you can trace back each operation back to\n",
    "$x$ from $y$\n",
    "\n",
    "To do this, you use the `GradientTape` to tell TensorFlow what are you interested\n",
    "in as $x$. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(5.0)\n",
    "\n",
    "# Inside the \"with\" call, we compute y from x\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x)\n",
    "    y = x ** 2\n",
    "    \n",
    "# After we have y, we can have the gradient dy/dx\n",
    "# which is \"recorded\" by the tape\n",
    "dydx = tape.gradient(y, x)\n",
    "\n",
    "print(dydx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the number make sense? You can try a few more times to see what it does.\n",
    "\n",
    "**TASK:** Try to find out:\n",
    "- Can a tape record many x variables?\n",
    "  (then we want the partial derivatives)\n",
    "- Can it compute second order gradients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We now have the power to compute gradients of any function, \n",
    "so long as we can compute that function in TensorFlow. \n",
    "The natural usage of this is to implement the gradient descent algorithm.\n",
    "*You have used it before, remember?*\n",
    "\n",
    "We showcase this with the simple linear regression problem, trying to fit some data with the function:\n",
    "$$y_\\mathrm{pred} = kx + b$$\n",
    "\n",
    "We try to minimize the loss function:\n",
    "$$\\mathrm{loss} = (y_\\mathrm{pred} - y_\\mathrm{data})^2$$\n",
    "\n",
    "Suppose the data we'd like to fit looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "x_data = np.linspace(0,10,20)\n",
    "y_data = x_data *2 + 1\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "The idea is simple, if we know the gradient of the loss function (that is, \n",
    "the error of prediction) with respect to the variables,\n",
    "we can minimize the error by going to the direction \n",
    "where loss function decreases.\n",
    "\n",
    "![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)\n",
    "\n",
    "To do that, we need to know the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "In the following block, two parameter $k$, $b$ and a loss function (the mean squared error) are defined.\n",
    "The following code calculates the gradient of the loss function with respect to $k$ and $b$.\n",
    "You need to complete the code by:\n",
    "\n",
    "**TASK:** \n",
    "- Record the gradient for $k$ and $b$.\n",
    "- Compute the prediction `y_pred` from `x_data`, `k` and `b`.\n",
    "- Inspect the gradients. Can you verify that they are correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.constant(4.0)\n",
    "b = tf.constant(2.0)\n",
    "\n",
    "# note we added persisten=True here\n",
    "# it allows you to do tape.gradient multiple times\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # You need to complet the code from here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # to here \n",
    "    loss = tf.reduce_mean((y_pred - y_data)**2)\n",
    "\n",
    "dlossdk = tape.gradient(loss, k)\n",
    "dlossdb = tape.gradient(loss, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the gradients to update the values of $k$ and $b$. In each step, \n",
    "we update the values of k and b according to $d\\mathrm{loss}/dk$ and $d\\mathrm{loss}/db$.\n",
    "\n",
    "**TASK:**\n",
    "- In the next code block, compute the derivatives of the loss function inside the `for` loop. (you can reuse your previous code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = tf.constant(2.0)#\n",
    "b = tf.constant(2.0)#\n",
    "\n",
    "max_steps = 1000\n",
    "step = 0.01\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # You need to complete the code from here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # to here\n",
    "    k = k - step * dlossdk\n",
    "    b = b - step * dlossdb\n",
    "    if i%100 == 0:\n",
    "        print(f'Step={i}, k={k:.2f}, b={b:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code block below to see how the resulting model looks like.\n",
    "\n",
    "Did it work? Remember that gradient descent can be unstable when the step is large, \n",
    "modify the `max_steps` and `step` to see if it helps?\n",
    "\n",
    "**BONUS**: store the loss function of each step and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_pred)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "Of course, we are not satisfied with linear regression,\n",
    "or any regression with a known function. \n",
    "In the real work scenario we cannot even have a good guess of how a function looks like.\n",
    "This is when neural network could save the day.\n",
    "\n",
    "\n",
    "First, let's define our \"very complex\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.linspace(0,10,20)\n",
    "y_data = np.sin(x_data)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The structure\n",
    "The idea of a single-layer neural network is that we take the input\n",
    "variable, transform it into several hidden units with a linear\n",
    "function and an activation function. And predict the output\n",
    "as a linear combination of the hidden layers.\n",
    "\n",
    "<img width=800px src=\"https://miro.medium.com/max/1000/1*_7Om4rgZytZe10fXUZkNCA.png\" \n",
    "     style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "### The operations\n",
    "\n",
    "Mathematically, this means you need `n_hidden` $k_1$ values called\n",
    "\"weights\", and `n_hidden` $b$ values called \"biases\" to transform the \n",
    "input to hidden units. Then `n_hidden` $k_2$ values and one $b_2$\n",
    "to get the output.\n",
    "\n",
    "Written in the vector form:\n",
    "- $\\vec{\\mathrm{hidden}} = \\mathrm{tanh}(\\vec{k_1}\\cdot x+\\vec{b_1})$\n",
    "- $y = \\vec{k_2}^\\top \\cdot \\vec{\\mathrm{hidden}}+b_2$\n",
    "\n",
    "\n",
    "### The activation function\n",
    "The activation function is an essential part of neural networks, without an activation, this neural network is just a plain linear regression. It gets its name from the function used to approximate how a extracellular field affects (activates) neurons.\n",
    "\n",
    "You can see the activation function as a \"switch\", which controls \n",
    "when a \"hidden unit\" in the neural network is \"ON\" or \"OFF\", depending on its input value. \n",
    "The activation function we use here the *tanh* function, \n",
    "it looks like this:\n",
    "\n",
    "<img width=400px src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1280px-Hyperbolic_Tangent.svg.png\" \n",
    "     style=\"width: 400px;\"/>\n",
    "          \n",
    "\n",
    "An intuitive explanation of how this works is that tanh outputs \"almost\" a linear function between [-0.5, 0.5], and constants when the input is large. It means one hidden unit only \"activates\" between certain regions of the input. \n",
    "\n",
    "When we have a lot of hidden units, we can fit a complex function by having each of the hidden unit \"activating\" at specific input range. By combining those hidden units, we can fit a function with any shape.\n",
    "\n",
    "TensorFlow has provided a [demo](https://playground.tensorflow.org/) where you can play with neural networks with different activation functions are layo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a single-layer neural network\n",
    "\n",
    "Now we'll implement such a neural network.\n",
    "\n",
    "First we try to compute the hidden layer from $k_1$ and $b_1$:\n",
    "\n",
    "You may see an error saying that the two shapes are incompatible.\n",
    "This is because you want to compute the 10 multiplications \n",
    "for every data you have in the 20 samples. \n",
    "\n",
    "You therefore need a 2-d matrix with the shape (20 by 10) to represent the hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_1 = tf.random.normal([10])\n",
    "b_1 = tf.random.normal([10])\n",
    "\n",
    "\n",
    "print(x_data.shape, k_1.shape)\n",
    "print(x_data * k_1 + b_1)\n",
    "# ^^^^^ This fails ^^^^\n",
    "\n",
    "print(x_data[:, None].shape, k_1[None, :].shape)\n",
    "hidden = x_data[:, None] * k_1[None, :] + b_1[None, :]\n",
    "print(hidden.shape)\n",
    "# ^^^^ This will work ^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter lines works because TensorFlow (also numpy) broadcasts arrays automatically.  \n",
    "You can read more here: https://www.tensorflow.org/xla/broadcasting.\n",
    "\n",
    "Now try to complete the following block (hint: you'll need an activation function like [`tf.tanh`](https://www.tensorflow.org/api_docs/python/tf/math/tanh) and also [`tf.reduce_sum`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum) for adding up the hidden unit outputs, click on the functions to see their documentations if you are not sure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = tf.random.normal([25])\n",
    "b1 = tf.random.normal([25])\n",
    "k2 = tf.random.normal([25])\n",
    "b2 = tf.random.normal([1])\n",
    "\n",
    "def neural_net(x, k1, b1, k2, b2):\n",
    "    # You need to complete the code from here\n",
    "\n",
    "    # to here\n",
    "    return y_pred\n",
    "\n",
    "y_pred = neural_net(x_data, k1, b1, k2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of the weights\n",
    "\n",
    "You might notice that we initialize the $k$ and $b$ vairiables with `tf.random.normal`. It means that we randomly select their values from a normal distribution. \n",
    "\n",
    "**TASK:**\n",
    "- Think about why this is necessary.\n",
    "- Can we just set $k_1$ to be random? What about $k_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this you can use our previous gradient descent loop to train your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 1000\n",
    "step = 0.001\n",
    "\n",
    "# You don't need to change the code below,\n",
    "# but feel free to play with the step settings\n",
    "# and number of hidden layers\n",
    "k1 = tf.random.normal([25])\n",
    "b1 = tf.random.normal([25])\n",
    "k2 = tf.random.normal([25])\n",
    "b2 = tf.random.normal([1])\n",
    "\n",
    "for i in range(max_steps):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(k1)\n",
    "        tape.watch(b1)\n",
    "        tape.watch(k2)\n",
    "        tape.watch(b2)\n",
    "        \n",
    "        y_pred = neural_net(x_data, k1, b1, k2, b2)\n",
    "        loss = tf.reduce_mean((y_pred - y_data)**2)\n",
    "    \n",
    "    dk1 = tape.gradient(loss, k1)\n",
    "    db1 = tape.gradient(loss, b1)\n",
    "    dk2 = tape.gradient(loss, k2)\n",
    "    db2 = tape.gradient(loss, b2)\n",
    "    k1 = k1 - step * dk1\n",
    "    b1 = b1 - step * db1\n",
    "    k2 = k2 - step * dk2\n",
    "    b2 = b2 - step * db2\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(f'Step={i}, loss={loss:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_pred)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:** You might see a bad fit, you may now tune the number of hidden units,  \n",
    "the steps of the gradient descent to see if it improves the performance.\n",
    "\n",
    "**Note** that you are also allowed to add more points to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing your neural network\n",
    "\n",
    "**TASK:** After you feel happy about your neural network, you should test it \n",
    "on data that it has not seen. This is called a test (or validation) set.\n",
    "\n",
    "- Does your neural network work well?\n",
    "- What if you try to predict beyond the range you have trained it on?  \n",
    "  (try change `x_test` to something below 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(0,10,1000)\n",
    "y_test = np.sin(x_test)\n",
    "y_pred_test = neural_net(x_test, k1, b1, k2, b2)\n",
    "\n",
    "plt.plot(x_test, y_test, 'r-')\n",
    "plt.plot(x_test, y_pred_test, 'k--')\n",
    "\n",
    "plt.legend(['test set label', 'test set prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural network\n",
    "\n",
    "Although it can be proven that a single layer neural network can fit **any** continuous function given enough data, the raise of neural network is due to the advance of deep neural networks (or so-called deep learning).\n",
    "\n",
    "![](https://www.researchgate.net/profile/Martin_Musiol/publication/308414212/figure/fig1/AS:409040078295040@1474534162122/A-general-model-of-a-deep-neural-network-It-consists-of-an-input-layer-some-here-two.png)\n",
    "\n",
    "The idea is rather simple: if transforming the input into a set of non-linear hidden units improves our model compared to a linear one, can we improve more by transforming the hidden units with yet another hidden layer? These models are conceptually possible, but we lack feasible ways to train the parameters.\n",
    "\n",
    "Since we have now $n_\\mathrm{hidden1} \\times n_\\mathrm{hidden2}$ weights to transform hidden layer 1 to hidden layer 2, the number of parameter explodes with number of hidden layers. \n",
    "\n",
    "Thanks to the development of libraries like TensorFlow (to compute the gradients for very deep models), gradient descent (to update the parameters but avoid oscillations)\n",
    "as well as the increasing compute power, the deep neural networks are now much easier to train (or to optimize the parameters). \n",
    "\n",
    "\n",
    "**BONUS:** You are now able try to implement a multi-layer neural network using what we've learnt so far. It's Ok if you don't, TensorFlow provides handy functions to help you create deep neural networks efficiently. After the break, we will build \n",
    "a deep neural network with the Keras API.\n",
    "\n",
    "Here are some tips if you're going to try:\n",
    "- Mind the dimensions: at this point you will have to do many matrix multiplications, \n",
    "  it is important that you do the then properly and the dimension matches.\n",
    "- Define a function that does one gradient descent step and gives: `new_params = grad_descent(old_params, x_data, y_data)`.\n",
    "- After you're done, wrap the above function with the [`@tf.fucntion`\n",
    "  decorator](https://www.tensorflow.org/api_docs/python/tf/function). TensorFlow will automatically optimize your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No hints here\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
