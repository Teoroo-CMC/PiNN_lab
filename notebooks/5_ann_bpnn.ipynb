{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atomic Neural Networks\n",
    "\n",
    "As we already have the universal function fitter -- deep neural networks,\n",
    "we should be able to fit any property of atoms, molecules or materials, right?\n",
    "\n",
    "Not so fast, Unlike our previous example of training a single-variable function, \n",
    "our input is a sparse collection of atomic positions and atoms.\n",
    "How can we train the parameters, if the input size varies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The representation problem\n",
    "\n",
    "The first approach we'll introduce is to see this as a representation problem.\n",
    "If we find out a way to encode the structure as a fixed size input, we can \n",
    "apply neural network as we have done before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coulomb matrix\n",
    "\n",
    "Early attempts in this direction often require the whole atomic structure (e.g. Coulomb matrix, internal coordinates as input). This works fine if our system is fixed, for example, if we are looking at the trajectory of a certain molecule. In this example, any possible structure of the benzene molecule can be uniquely described as a matrix of the atomic distances.\n",
    "\n",
    "![image.png](https://www.mrupp.info/Graphics/fig_qmml.png)\n",
    "\n",
    "Although these approaches were found to be useful for describing certain reaction, it is limited for more general purposes, as the input-size must be fixed.\n",
    "Imagine you would like to train the network on a dataset of small molecules, and try to predict about large systems, or train the network with MD data of water clusters and simulate bulk water, you will not be able \n",
    "to describe the large system and the small system with the same input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atomic decomposition\n",
    "\n",
    "One solution of to address this problem is to predict on **atoms** instead of the **system**.\n",
    "Furthermore, one only describe the atom with its environment (within a certain cutoff range).\n",
    "This is a reasonable approximation in most cases, and allows one to decease the amount of computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetry functions\n",
    "\n",
    "One of such **atomic description** is given by symmetry functions. \n",
    "Used in the Behler Parrinello Neural Network (BPNN) $^{[1]}$\n",
    "The word symmetry refers to the symmetries that an atomic description should obey.\n",
    "\n",
    "- Rotation of the system\n",
    "- The translation of the system\n",
    "- The permutation of nearby atoms\n",
    "\n",
    "![](https://amp.readthedocs.io/en/latest/_images/gaussian.svg)\n",
    "\n",
    "Behler's symmetry function meets the three requirements by \n",
    "1. Calculating a function for each pair of atoms,  \n",
    "   those functions does not change with the translation or rotation of the system.\n",
    "2. adds the function up to a central atom as a description of the atom,  \n",
    "   so that the description is invariant to the order of atoms.\n",
    "\n",
    "Essentially, this correspond to a transformation of the radial distribution function of the central atoms. Similar approach can be used with angular distribution or even dihedral distributions, to improve the atomic description.\n",
    "\n",
    "### BPNN\n",
    "\n",
    "![](https://pubs.rsc.org/image/article/2017/SC/c7sc02267k/c7sc02267k-f1_hi-res.gif)\n",
    "\n",
    "BPNN then use those symmetry functions as inputs of the neural network. One more trick here is to use a separate set of symmetry function for each type of atom, and then use a separate neural network to make the prediction. This allows the description as well as the network to be tailored for each type of element and  improves the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]\n",
    "J. Behler and M. Parrinello, “Generalized Neural-Network Representation of High-Dimensional Potential-Energy Surfaces,” Physical Review Letters, vol. 98, no. 14, Apr. 2007, doi: 10.1103/physrevlett.98.146401"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
