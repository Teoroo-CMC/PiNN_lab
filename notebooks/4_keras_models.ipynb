{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a deep neural network with Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level (meaning highly abstracted and easy to use)\n",
    "Python library for building deep neural networks. It is integrated in TensorFlow so you\n",
    "can directly call it from `tf.keras`.\n",
    "\n",
    "Keras abstracts the concepts of layers, models and training, it keeps track of your \n",
    "variable and gradients automatically and efficiently so you don't have to write your\n",
    "functions and loops manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.linspace(0,10,20)[:,None]\n",
    "y_data = np.sin(x_data)\n",
    "plt.plot(x_data, y_data, 'rx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layer\n",
    "\n",
    "In Keras a hidden layer including a linear transformation and an activation is called a \n",
    "dense layer.  \n",
    "You can build one like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 specifies how many units the layer outputs\n",
    "layer1 = tf.keras.layers.Dense(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a layer like a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = layer1(x_data)\n",
    "# To output y, just use a dense layer that outputs 1 unit\n",
    "layer2 = tf.keras.layers.Dense(1)\n",
    "y_pred = layer2(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras model\n",
    "\n",
    "Keras model is a easy way to define and train you model.\n",
    "\n",
    "To build a model, you write how it should compute the output from a input.  \n",
    "(note that `tf.keras.Input` is a placeholder for your input)\n",
    "\n",
    "Then Keras will figure out what your model looks like provided your \n",
    "input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(1,))\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(10, activation='tanh')\n",
    "layer2 = tf.keras.layers.Dense(10, activation='tanh')\n",
    "layer4 = tf.keras.layers.Dense(1)\n",
    "\n",
    "hidden1 = layer1(inputs)\n",
    "hidden2 = layer2(hidden1)\n",
    "y_pred = layer4(hidden2)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the information of your model with this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Keras model is easy\n",
    "\n",
    "- Define the loss function and the optimization algorithm with `model.compile`\n",
    "- Run the fitting with `model.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='MSE', optimizer='Adam')\n",
    "model.fit(x_data, y_data, epochs=5000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a Keras model is also easy\n",
    "- Just use `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_data)\n",
    "x_test = np.linspace(-1,11,1000)\n",
    "y_test = np.sin(x_test)\n",
    "y_pred_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data, y_data, 'rx')\n",
    "plt.plot(x_data, y_pred, 'g-')\n",
    "plt.plot(x_test, y_test, 'b-')\n",
    "plt.plot(x_test, y_pred_test, 'k--')\n",
    "plt.legend([\n",
    "  'train set label', 'train set prediction',  \n",
    "  'test set label', 'test set prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now easily design and train a deep neural network, \n",
    "and you know what's happening under the hood. \n",
    "\n",
    "Here we have a last tool for you to aid your training.\n",
    "\n",
    "## TensorBoard\n",
    "\n",
    "TensorBoard is a visualization tool provided with TensorFlow. \n",
    "It allows you log your training metrics, inspect the \n",
    "performance and structure of your models, and more...\n",
    "\n",
    "To use it, you need to add a \"callback\" to your model. \n",
    "Which tells Keras to log the data during your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/my_deep_neural_net/\" \n",
    "callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, write_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the callback during the fitting\n",
    "\n",
    "We do one more things here: we use the\n",
    "`validation_data` option to tell Keras to validate \n",
    "on the test set automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model.compile(loss='MSE', optimizer='Adam')\n",
    "model.fit(x_data, y_data, epochs=1000,\n",
    "          validation_data=(x_test, y_test),\n",
    "          verbose=0, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, launch TensorBoard to see your training log.\n",
    "\n",
    "**Remember** to change your logdir whenever you train a new model,\n",
    "otherwise they will be grouped as the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:**\n",
    "\n",
    "Now you may use the Keras model and TensorBoad to compare different\n",
    "models, try a few different setups and see if you see an improvement.\n",
    "\n",
    "- Change the number of layers and hidden units in your model  \n",
    "  name the log directory properly and visualize the label and prediction of the model\n",
    "- Try to overfit the model  \n",
    "  Neural networks can be unstable when the training set is small, try to verify this\n",
    "  - Do you see a large difference between training and test set error?\n",
    "  - **BONUS:** Do you see unrealistic curves in your prediction?\n",
    "    What if you add some random noise?  (see [np.random](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html))\n",
    "  - **BONUS:** So far we've only used the `tanh` hyperbolic tangent function but there are [many other options](  https://en.wikipedia.org/wiki/Activation_function). See if they make a difference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
